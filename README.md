# Generated Text Classification Project

## About the Project

This project provides a comprehensive solution for detecting whether a given text is written by a human or generated by an AI model. With the proliferation of advanced language models, distinguishing between human and AI-generated content is increasingly important for academic integrity, content moderation, and combating misinformation. The project includes a full machine learning pipeline, from data analysis and preprocessing to model training, hyperparameter optimization, and deployment via a user-friendly web application.

## About the Dataset

The dataset used in this project is sourced from [Kaggle: AI vs Human Text](https://www.kaggle.com/datasets/shanegerami/ai-vs-human-text). This dataset contains a large collection of text samples, each labeled as either human-written or AI-generated. The data is balanced and diverse, featuring a variety of topics and writing styles. Each entry includes the text and a binary label indicating its origin. The dataset is well-suited for training and evaluating classification models, as it provides clear distinctions and real-world examples of both classes.

## Notebook Summary

The main notebook, `generated-text-classification.ipynb`, guides users through the following steps:

- **Problem Definition:** Outlines the objective of distinguishing between human and AI-generated text.
- **Exploratory Data Analysis (EDA):** Examines the dataset, visualizes class distributions, analyzes text length, lexical richness, punctuation, and stopword ratios.
- **Text Preprocessing:** Applies normalization, stopword removal, and noise filtering (removing URLs, emails, hashtags, mentions, numbers), and filters out texts that are too short or too long.
- **Model Selection:** Constructs a machine learning pipeline using `CountVectorizer`, `TfidfTransformer`, and `MultinomialNB` for initial classification.
- **Hyperparameter Tuning:** Uses Optuna to optimize n-gram ranges, TF-IDF usage, and Naive Bayes smoothing parameters.
- **Evaluation:** Presents classification metrics and saves the best-performing model for deployment.

## Model Results

### Preprocessing

- **Text Normalization:** Converts all text to lowercase, removes extra whitespace, and strips punctuation.
- **Stopword Removal:** Filters out common English stopwords to focus on meaningful words.
- **Noise Removal:** Eliminates URLs, email addresses, hashtags, mentions, and numbers to reduce irrelevant information.
- **Length Filtering:** Removes texts that are extremely short or long (based on quantiles for each class) to ensure consistent input for the model.

### Training

- **Feature Extraction:** Uses `CountVectorizer` to transform text into word count vectors, followed by `TfidfTransformer` to reweight features based on their importance across the corpus.
- **Model:** Employs a `MultinomialNB` classifier, which is efficient and effective for text classification tasks.
- **Hyperparameter Optimization:** Utilizes Optuna to search for the best n-gram range, TF-IDF usage, and smoothing parameter (`alpha`) for the Naive Bayes model.

### Evaluation

- **Metrics:** Evaluates the model using accuracy, precision, recall, and F1-score on a held-out test set.
- **Results:** The optimized model demonstrates high accuracy, with detailed classification reports showing strong performance in distinguishing between human and AI-generated texts.
- **Model Saving:** The best pipeline is saved using `joblib` for easy reuse in the web application.

## How to Install

Follow these steps to set up the project in a virtual environment:

```bash
# Clone the repository or download the project files
git clone https://github.com/DeepActionPotential/TextTector
cd TextTector

# Create a virtual environment
python -m venv venv

# Activate the virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate

# Upgrade pip
pip install --upgrade pip

# Install required packages
pip install -r requirements.txt


```

## How to Use the Software

1. **Start the Web Application:**
   ```bash
   streamlit run app.py
   ```
2. **Demo:**
   ## [Video-Demo](images/texttector-demo.mp4)

   ![image](images/1.png)
   -------
   ![image](images/2.png)


## Technologies Used


- **Pandas & NumPy:** For data manipulation and analysis.
- **Seaborn & Matplotlib:** For data visualization during EDA.
- **NLTK:** For natural language processing tasks, especially stopword removal.
- **Scikit-learn:** Provides the machine learning pipeline, feature extraction (`CountVectorizer`, `TfidfTransformer`), model training (`MultinomialNB`), and evaluation tools.
- **Optuna:** For automated hyperparameter optimization, enabling efficient search for the best model configuration.
- **Joblib:** For model serialization and deployment.
- **Streamlit:** For building the interactive web application that allows users to check text in real time.

## License

This project is licensed under the MIT License. You are free to use, modify, and distribute this software for personal or commercial purposes, provided that you include the original copyright and license.
